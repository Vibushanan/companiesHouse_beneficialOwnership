{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "The base containers include:\n",
    "\n",
    "- a PostgreSQL database\n",
    "- a Jupyter notebook server containing a range of scientific python packages;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get these scripts up and running, you will need to install Docker on your computer or be able to run Docker containers on a third party cloud server. \n",
    "\n",
    "\n",
    "Note - there seems to be a problem running newly downloaded versions of docker on Macs:-(\n",
    "\n",
    "\n",
    "The following `docker-compose.yml` file contents will download and install several useful containers. (Containers are like self-contained applications that run inside a virtual machine on your computer - they shouldn't upset or unnecessarily interact with any other software on your machine...)\n",
    "\n",
    "    #----------------- START docker-compose.yml ------------------\n",
    "    #mongodata:\n",
    "    #    image: busybox\n",
    "    #    volumes: \n",
    "    #        - /data/db\n",
    "\n",
    "    #mongodbdd:\n",
    "    #    image: mongo\n",
    "    #    ports:\n",
    "    #        - \"27107:27107\"\n",
    "    #    volumes_from:\n",
    "    #        - mongodata\n",
    "    #    command: --smallfiles\n",
    "\n",
    "    postgresdatadd:\n",
    "        image: busybox\n",
    "        volumes: \n",
    "            - /var/lib/postgresql/data\n",
    "\n",
    "    postgresdd:\n",
    "        image: postgres #mdillon/postgis \n",
    "        environment:\n",
    "            - POSTGRES_PASSWORD=PGPass\n",
    "        ports:\n",
    "            - \"5432:5432\"\n",
    "        volumes_from:\n",
    "            - postgresdatadd\n",
    "\n",
    "    #neo4jdd:\n",
    "    #  image: neo4j\n",
    "    #  ports:\n",
    "    #    - \"7474:7474\"\n",
    "    #    - \"1337:1337\"\n",
    "    #  volumes:\n",
    "    #    - /opt/data\n",
    "\n",
    "    datadive:\n",
    "        image: jupyter/scipy-notebook\n",
    "        user: root\n",
    "        environment:\n",
    "            - GRANT_SUDO=yes\n",
    "        ports:\n",
    "            - \"9988:8888\"\n",
    "        links:\n",
    "            - postgresdd:postgres\n",
    "            #- mongodbdd:mongodb\n",
    "            #- neo4jdd:neo4j\n",
    "        volumes:\n",
    "            - .:/home/jovyan/work\n",
    "\n",
    "    #----------------- END docker-compose.yml ------------------\n",
    "\n",
    "Download and install Kitematic, a GUI for working with containers.\n",
    "\n",
    "From the bottom right hand corner of Kitematic you can open a Docker command line.\n",
    "\n",
    "![](kitematic-dd.png)\n",
    "\n",
    "\n",
    "On the Docker command line, change directory to the directory containing the `docker-compose.yml` file and run the command:\n",
    "\n",
    "docker-compose up -d\n",
    "\n",
    "This will launch a database container linked to a Jupyter notebook server. You should be able to launch it from within Kitematic.\n",
    "\n",
    "To stop the containers: docker-compose stop\n",
    "To start the containers back up again: docker-compose start\n",
    "\n",
    "To destroy the containers and their contents: docker-compose down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook container is lacking some libraries and packages we need to make life easier... The `-qq` and `--quiet` flags make the install a quiet one...\n",
    "\n",
    "To run the code cell, click on it and press the run button in the tool bar at the top of the notebook or press *shift-return*. (See other keyboard shortcuts from the notebook *Help* menu.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sudo apt-get -qq update   && sudo apt-get -qq install  -y libpq-dev python-dev\n",
    "!pip install --quiet ipython-sql psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebooks support SQLMagic. This lets us run SQL commands in a code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: postgres@postgres'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext sql\n",
    "#This is how we connect to a sql database\n",
    "#Monolithic VM addressing style\n",
    "%sql postgresql://postgres:PGPass@postgres:5432/postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run SQL commands on the database using the `pandas` python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\"postgresql://postgres:PGPass@postgres:5432/postgres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the basic orientation data\n",
    "\n",
    "Loading CSV data directly into a PostgreSQL database often required the CSV to be well behaved; *pandas* is a bit more forgiving on the import and can be used to load the data from the original CSV file and then insert it into the PostgreSQL database. As *pandas* dataframes are stored in memory we need to load the data in from the source file in chunks of several thouasnds of lines at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the path to the sample data\n",
    "#Look for it in the current directory:companies-with-controlling-entities v6.csv\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If it's not in the current directory, also specify the path to the file\n",
    "datafile=\"companies-with-controlling-entities v6.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preview the first 5 rows of the file contents\n",
    "!head -n 5 \"$datafile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easier working with the data, we'll use *pandas*, a library descigned for working with tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the data from the CSV file\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pandas* dataframes are held in memory, which can cause your machine to fall over if the datafile is a large one.\n",
    "\n",
    "To work with the data, we can load it into a database.\n",
    "\n",
    "*pandas* can create a database table, and populate it, from a dataframe without you haveing to create the database table first.\n",
    "\n",
    "The PostgreSQL database table will be defined based on the dataframe column names properties - we may need to force column datatypes when we read in the datafile but for now we'll go with whatever the defaults turn out to be...\n",
    "\n",
    "Rather than load a possibly stupidly large database into a dataframe and from their into the database, *pandas* will let you load the data a bit at a time, chunking it a few hundred or thouasand rows at a time, creating the database table for the first set of rows and then appending the later ones.\n",
    "\n",
    "Balance the chunksize with speed (it can be quicker to load lots of rows at once) and not making your machine fall over (becuase you've loaded too many rows into memory via a *pandas* dataframe) at the same time.\n",
    "\n",
    "If the machine hands for too long, restart the python kernel (from the notebook *Kernel* menu). If that doesn't work, select and *Shutdown* the notebook from the notebook homepage.\n",
    "\n",
    "Note that the data ingest may take some time - go and grab a coffee once you start to run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the base data into a table called: sigcontrol\n",
    "#Drop the table if it already exists so we start from a blank state\n",
    "%sql DROP TABLE IF EXISTS sigcontrol\n",
    "\n",
    "#Load the data in ten thouand rows at a time\n",
    "chunks=pd.read_csv(datafile,chunksize=10000,dtype=str)\n",
    "\n",
    "#This may take some time - go an grab a coffee....\n",
    "\n",
    "#Note that we can also read in from a zip file - as long as it isn't password protected:\n",
    "#pd.read_csv(fn,chunksize=10000,dtype=str, compression='gzip')\n",
    "for chunk in chunks:\n",
    "    #Pop the data into the \n",
    "    chunk.to_sql('sigcontrol',engine,index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the data to check the ingest worked..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM sigcontrol LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be good to go...\n",
    "\n",
    "Try out the exercises in the orientiation notebook to start to familiarise yourself with the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
